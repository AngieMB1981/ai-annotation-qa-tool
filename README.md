# AI Annotation Quality Review Tool

## Overview
This project demonstrates a simple Python-based quality assurance tool for reviewing AI annotation data. It applies common QA rules used in annotation workflows to identify potential issues before data is used for AI training.

## What the Tool Does
- Flags missing labels
- Flags invalid labels not in an allowed set
- Identifies low-quality text entries that are too short

## Skills Demonstrated
- AI data annotation and labeling concepts
- Quality assurance and data validation
- Python fundamentals (lists, dictionaries, conditionals, functions)
- Applying detailed guidelines consistently

## Example Use Case
This tool mirrors entry-level AI annotation review tasks where accuracy, consistency, and rule-following are critical.

## Sample Output
- Go (text too short)
- OK (missing label)
- OK (text too short)
- Save (invalid label)

## Notes
This is a practice project designed to reflect real-world AI data annotation and QA review workflows.
